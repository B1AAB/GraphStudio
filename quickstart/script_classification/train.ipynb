{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f9e16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# this must be here before cuda init\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3acf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f222a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from tqdm.notebook import tqdm\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.optim.lr_scheduler import SequentialLR, LinearLR, CosineAnnealingLR\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from script_classification.utilities.graph_stats import *\n",
    "from script_classification.data_loader import BitcoinScriptsDataset\n",
    "from script_classification.view_augmenter import ViewAugmenter\n",
    "from script_classification.models import GraphEncoder\n",
    "from script_classification.engine import *\n",
    "from script_classification.losses import *\n",
    "from script_classification.utilities.graph_ops import *\n",
    "from script_classification.evaluation.metrics import *\n",
    "from script_classification.evaluation.embeddings import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68839ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 64\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6031ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396f4a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = json.load(open(\"config.json\"))\n",
    "DATA_ROOT = config[\"data_root\"]\n",
    "SAVES_ROOT = config[\"saves_root\"]\n",
    "MODEL_SAVE_FILENAME = config[\"model_save_filename\"]\n",
    "MODEL_SAVE_PATH = os.path.join(SAVES_ROOT, MODEL_SAVE_FILENAME)\n",
    "LOGS_ROOT = os.path.join(SAVES_ROOT, \"logs_small\")\n",
    "MIN_NODES_PER_GRAPH = config[\"min_nodes_per_graph\"]\n",
    "MAX_NODES_PER_GRAPH = config[\"max_nodes_per_graph\"]\n",
    "BATCH_SIZE = config[\"batch_size\"]\n",
    "EPOCHS = config[\"epochs\"]\n",
    "WARMUP_EPOCHS = config[\"warmup_epochs\"]\n",
    "LEARNING_RATE = config[\"learning_rate\"]\n",
    "HIDDEN_DIM = config[\"hidden_dim\"]\n",
    "OUT_DIM = config[\"out_dim\"]\n",
    "TEMPERATURE = config[\"temperature\"]\n",
    "LAMBDA_VAL = config[\"lambda_val\"]  # Weight for the graph-level loss\n",
    "PATIENCE = config[\"patience\"]\n",
    "PROJ_DIM = config[\"proj_dim\"]\n",
    "NUM_CLUSTERS = config[\"num_clusters\"]\n",
    "KNN_K = config[\"knn_k\"]\n",
    "BLEND_ALPHA = config[\"blend_alpha\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5239f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = BitcoinScriptsDataset(\n",
    "    root=DATA_ROOT, \n",
    "    min_nodes_per_graph=MIN_NODES_PER_GRAPH, \n",
    "    max_nodes_per_graph=MAX_NODES_PER_GRAPH\n",
    ")\n",
    "\n",
    "graph_id_to_idx_map = {}\n",
    "for i in range(len(dataset)):\n",
    "    d = dataset.get(i)\n",
    "    gid = d.graph_id\n",
    "    try:\n",
    "        key = gid\n",
    "        graph_id_to_idx_map[key] = i\n",
    "    except TypeError:\n",
    "        graph_id_to_idx_map[str(gid)] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77ec1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "EDGE_DIM = getattr(dataset, \"num_edge_features\", dataset[0].edge_attr.size(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ff6fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(dataset)\n",
    "n_train = int(0.7 * n)\n",
    "n_val   = int(0.15 * n)\n",
    "n_test  = n - n_train - n_val\n",
    "\n",
    "gen = torch.Generator().manual_seed(SEED)\n",
    "train_raw, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    dataset, [n_train, n_val, n_test], generator=gen\n",
    ")\n",
    "\n",
    "train_dataset = train_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffeed65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a27a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = GraphEncoder(\n",
    "    in_channels=dataset.num_node_features,\n",
    "    hidden_channels=64, # divisible by HEADS\n",
    "    out_channels=OUT_DIM,\n",
    "    edge_dim=EDGE_DIM\n",
    ").to(device)\n",
    "\n",
    "\n",
    "proj_head = nn.Sequential(\n",
    "    nn.Linear(encoder.out_dim, PROJ_DIM),\n",
    "    nn.GELU(),\n",
    "    nn.Linear(PROJ_DIM, PROJ_DIM),\n",
    ").to(device)\n",
    "\n",
    "\n",
    "# AdamW + no weight decay on norm/bias\n",
    "decay, no_decay = [], []\n",
    "for n, p in encoder.named_parameters():\n",
    "    if not p.requires_grad: \n",
    "        continue\n",
    "    if n.endswith(\"bias\") or \"norm\" in n.lower():\n",
    "        no_decay.append(p)\n",
    "    else:\n",
    "        decay.append(p)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    [\n",
    "        {\"params\": encoder.parameters(), \"weight_decay\": 1e-4},\n",
    "        {\"params\": proj_head.parameters(), \"weight_decay\": 1e-4}\n",
    "    ], \n",
    "    lr=LEARNING_RATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e833f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "warmup_scheduler = LinearLR(optimizer, start_factor=1e-2, total_iters=WARMUP_EPOCHS)\n",
    "decay_scheduler  = CosineAnnealingLR(optimizer, T_max=max(2, EPOCHS - WARMUP_EPOCHS), eta_min=LEARNING_RATE * 0.1)\n",
    "scheduler = SequentialLR(optimizer, schedulers=[warmup_scheduler, decay_scheduler], milestones=[WARMUP_EPOCHS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d4401e",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_writer = SummaryWriter(LOGS_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5020fe89",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmenter = ViewAugmenter(\n",
    "    block_height_col=1,\n",
    "    block_height_scale_range=(0.99, 1.01),\n",
    "    block_height_shift_range=(-3.0, 3.0),\n",
    "    degree_cols=(2, 3),\n",
    "    degree_jitter=0.05,\n",
    "    value_col=0,\n",
    "    value_jitter=0.05\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fcf650",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_root_clustering(encoder, loader, device, n_clusters=10):\n",
    "    Z = embed_roots(encoder, loader, device)  # [G, D], NumPy\n",
    "    # in case someone swaps embed_roots to return a tensor:\n",
    "    if hasattr(Z, \"detach\"):\n",
    "        Z = Z.detach().cpu().numpy()\n",
    "    if Z.shape[0] < n_clusters:\n",
    "        return -1.0, None, None\n",
    "    km = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    labels = km.fit_predict(Z)\n",
    "    score = silhouette_score(Z, labels, metric=\"cosine\")\n",
    "    return score, Z, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa428998",
   "metadata": {},
   "outputs": [],
   "source": [
    "T_START, T_END = 0.5, 0.08\n",
    "\n",
    "best_val = float(\"-inf\")\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in tqdm(range(1, EPOCHS + 1)):\n",
    "    train_out = train_one_epoch(\n",
    "        encoder=encoder,\n",
    "        loader=train_loader,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        epoch=epoch,\n",
    "        writer=summary_writer,\n",
    "        augmenter=augmenter,\n",
    "        temp_root=max(T_END, T_START - (T_START - T_END) * (epoch-1) / max(1, EPOCHS-1)),\n",
    "        proj_head=proj_head\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        current_lr = scheduler.get_last_lr()[0] if hasattr(scheduler, \"get_last_lr\") else optimizer.param_groups[0][\"lr\"]\n",
    "    except Exception:\n",
    "        current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "    summary_writer.add_scalar(\"lr\", float(current_lr), epoch)\n",
    "\n",
    "    sil_score, _, _ = evaluate_root_clustering(\n",
    "        encoder=encoder,\n",
    "        loader=val_loader,\n",
    "        device=device,\n",
    "        n_clusters=NUM_CLUSTERS\n",
    "    )\n",
    "    \n",
    "    knn_consistency = evaluate_root_knn_consistency(\n",
    "        encoder=encoder,\n",
    "        loader=val_loader,\n",
    "        device=device,\n",
    "        augmenter=augmenter,\n",
    "        k=KNN_K\n",
    "    )\n",
    "\n",
    "    val_metric = knn_consistency if sil_score < 0 else BLEND_ALPHA * sil_score + (1.0 - BLEND_ALPHA) * knn_consistency\n",
    "    \n",
    "    tr_loss = float(train_out.get(\"loss\", float(\"nan\")))\n",
    "    tr_root_loss = float(train_out.get(\"root_loss\", float(\"nan\")))\n",
    "\n",
    "    summary_writer.add_scalar(\"val/silhouette_root\", float(sil_score), epoch)\n",
    "    summary_writer.add_scalar(\"val/knn_consistency_root\", float(knn_consistency), epoch)\n",
    "    summary_writer.add_scalar(\"val/metric\", float(val_metric), epoch)\n",
    "    summary_writer.add_scalar(\"train/total_loss_epoch\", tr_loss, epoch)\n",
    "    summary_writer.add_scalar(\"train/root_loss_epoch\",  tr_root_loss, epoch)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch:02d}/{EPOCHS}\\t|\\t\"\n",
    "        f\"Train loss {tr_loss:.4f} \"\n",
    "        f\"(root loss {tr_root_loss:.4f})\\t|\\t\"\n",
    "        f\"Val {val_metric:.4f} (sil {sil_score:.4f}, knn {knn_consistency:.4f})\\t|\\t\"\n",
    "        f\"LR {current_lr:.6f}\"\n",
    "    )\n",
    "\n",
    "    if val_metric > best_val:\n",
    "        best_val = val_metric\n",
    "        torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state\": encoder.state_dict(),\n",
    "            \"optimizer_state\": optimizer.state_dict(),\n",
    "            \"best_val_metric\": float(best_val),\n",
    "            \"silhouette\": float(sil_score),\n",
    "            \"knn_consistency\": float(knn_consistency),\n",
    "        }, MODEL_SAVE_PATH)\n",
    "        print(f\"New best model saved (val metric={best_val:.4f})\\n\")\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"Patience {patience_counter}/{PATIENCE}\\n\")\n",
    "        if patience_counter >= PATIENCE:\n",
    "            print(f\"\\nEarly stop: no improvement in {PATIENCE} epochs.\\n\")\n",
    "            break\n",
    "\n",
    "print(\"\\n\\nSuccessfully finished training!!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
