{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b2733de",
   "metadata": {
    "id": "6b2733de"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/B1AAB/GraphStudio/blob/main/g101/g101.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed3e340-17fd-4b71-a98e-c776aa45d053",
   "metadata": {
    "id": "9ed3e340-17fd-4b71-a98e-c776aa45d053"
   },
   "source": [
    "# Get to Know Bitcoin Graph\n",
    "\n",
    "The Bitcoin Graph dataset and its accompanying suite of tools and resources are purpose-built to bridge the cryptocurrency and ML communities. It is discussed in detail in the following paper:\n",
    "\n",
    "> Vahid Jalili. (2025). **The Temporal Graph of Bitcoin Transactions.** _In Advances in Neural Information Processing Systems (NeurIPS)._\n",
    ">\n",
    "> https://arxiv.org/abs/2510.20028\n",
    "\n",
    "\n",
    "Briefly, the suite contains:\n",
    "\n",
    "- ETL pipeline, [EBA](https://github.com/B1AAB/EBA):\n",
    "\n",
    "  - Interfaces with the Bitcoin network and parses every transaction recorded on the blockchain.\n",
    "\n",
    "  - Encodes transactions as graphs, yielding batched sets of nodes and edges in TSV files.\n",
    "\n",
    "  - Imports the batches into a Neo4j database for scalable querying of deep transactional context and entity relationships.\n",
    "\n",
    "  - Implements specialized sampling algorithms that generate sub-graphs for training graph neural network models.\n",
    "\n",
    "- [Graph Studio](https://github.com/B1AAB/GraphStudio):\n",
    "\n",
    "  - Showcases a suite of impactful community-developed applications on the graph.\n",
    "\n",
    "\n",
    "These resources are organized by application scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1h1d7TxH-MY4",
   "metadata": {
    "id": "1h1d7TxH-MY4"
   },
   "source": [
    "This notebook serves as a guided tour of the [Bitcoin Graph](https://registry.opendata.aws/eba) dataset, and contains the following sections.\n",
    "\n",
    "\n",
    "* The _Seeds_ ðŸŒ± **Granular Graph Data**:\n",
    "\n",
    "  ðŸƒ skim through raw data.\n",
    "\n",
    "\n",
    "* The _Trunk & Roots_ ðŸŒ³ **The Full Graph Database**:\n",
    "\n",
    "  ðŸƒ skim through loading data into a Neo4j database.\n",
    "\n",
    "* The _Branches_ ðŸŒ¿ **Sampled Communities**:\n",
    "\n",
    "  ðŸƒ skim through community sampling and pre-sampled _hello-world_ communities.\n",
    "\n",
    "* The _Fruits_ ðŸŽ **Building Models**:\n",
    "\n",
    "  ðŸƒ skim through model training, and\n",
    "  \n",
    "  ðŸš¶ focused model evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q_jwxa3oNNvw",
   "metadata": {
    "id": "q_jwxa3oNNvw"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "THFXInnDNRj5",
   "metadata": {
    "id": "THFXInnDNRj5"
   },
   "source": [
    "#### Install & Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "P5e3IQORM8tq",
   "metadata": {
    "collapsed": true,
    "id": "P5e3IQORM8tq"
   },
   "outputs": [],
   "source": [
    "!pip install boto3>=1.38.23\n",
    "!mkdir -p /tmp/b1aab/graphstudio\n",
    "!git clone https://github.com/B1AAB/GraphStudio /tmp/b1aab/graphstudio\n",
    "\n",
    "!pip install -r /tmp/b1aab/graphstudio/quickstart/script_classification/requirements.txt\n",
    "!pip install -e /tmp/b1aab/graphstudio/quickstart/script_classification\n",
    "!pip install kagglehub\n",
    "!pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KW5AbXT_Nbxo",
   "metadata": {
    "id": "KW5AbXT_Nbxo"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "pkg_path = '/tmp/b1aab/graphstudio/quickstart/script_classification'\n",
    "if pkg_path not in sys.path:\n",
    "    sys.path.insert(0, pkg_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ti7Y3wymNiFR",
   "metadata": {
    "id": "Ti7Y3wymNiFR"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import kagglehub\n",
    "from torch_geometric.loader import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import umap\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import numpy as np\n",
    "import json\n",
    "import gdown\n",
    "import tempfile\n",
    "import networkx as nx\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.config import Config\n",
    "\n",
    "from script_classification.data_loader import BitcoinScriptsDataset\n",
    "from script_classification.models import GraphEncoder\n",
    "from script_classification.evaluation.embeddings import embed_roots\n",
    "from script_classification.evaluation.clustering import find_optimal_clusters_root, plot_find_optimal_clusters_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2TYVncqENl2i",
   "metadata": {
    "id": "2TYVncqENl2i"
   },
   "outputs": [],
   "source": [
    "working_dir = tempfile.gettempdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "IObV0c2qNpM6",
   "metadata": {
    "id": "IObV0c2qNpM6"
   },
   "source": [
    "#### Configure Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1m4g9nU7NtLR",
   "metadata": {
    "id": "1m4g9nU7NtLR"
   },
   "outputs": [],
   "source": [
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cYLAWxNw79",
   "metadata": {
    "id": "f6cYLAWxNw79"
   },
   "outputs": [],
   "source": [
    "SEED = 64\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Z6qgb6b-YjIa",
   "metadata": {
    "id": "Z6qgb6b-YjIa"
   },
   "source": [
    "# The _Seeds_ ðŸŒ± **Granular Graph Data**\n",
    "\n",
    "The Bitcoin graph dataset provides a complete history of transactions recorded on the Bitcoin blockchain, specifically structured for machine learning. The [v1 release](https://eba.b1aab.ai/releases/data-release/v1) release consists of over `2.4` billion nodes and `39.72` billion edges.\n",
    "\n",
    "\n",
    "In this release, the nodes and edges are provided in batched `.tsv.gz` files. Each batch contains nodes and edges representing transactions from a contiguous set of Bitcoin blocks. The batches are organized to have a relatively similar number of nodes and edges; consequently, the number of blocks covered by each batch varies.\n",
    "\n",
    "Each batch is structured as follows:\n",
    "\n",
    "* It contains one file for each node type and one file for each edge type.\n",
    "\n",
    "* File names are prefixed with a batch ID (the Unix timestamp when the ETL pipeline processed the batch) and the node or edge type, and they end with `.tsv.gz`.\n",
    "\n",
    "\n",
    "Additionally, since the dataset is prepared for easier import into a Neo4j database, the TSV files in each batch do not contain headers. Instead, there is a separate file for each node and edge type, without a batch prefix, that contains headers only.\n",
    "\n",
    "Moreover, there are three files named `0_BitcoinGraph.tsv.gz`, `unique_BitcoinScriptNode.tsv.gz`, and `unique_BitcoinTxNode.tsv.gz` that contain all unique block, script, and transaction nodes, respectively, across all batches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pl8rT2GohLgL",
   "metadata": {
    "id": "pl8rT2GohLgL"
   },
   "source": [
    "## Dataset overview\n",
    "\n",
    "In the following we list versions of the dataset hosted on AWS Open Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0q5GNiNPQOEf",
   "metadata": {
    "id": "0q5GNiNPQOEf"
   },
   "outputs": [],
   "source": [
    "bucket=\"bitcoin-graph\"\n",
    "s3 = boto3.client(\"s3\", config=Config(signature_version=UNSIGNED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_oQL8FtaQwAT",
   "metadata": {
    "id": "_oQL8FtaQwAT"
   },
   "outputs": [],
   "source": [
    "for item in s3.list_objects_v2(Bucket=bucket, Delimiter='/')['CommonPrefixes']:\n",
    "  print(f\"Dataset versions: {item['Prefix']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3A9uzg5YkFHN",
   "metadata": {
    "id": "3A9uzg5YkFHN"
   },
   "source": [
    "Objects count in the v1 of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SAB4tGfajlSo",
   "metadata": {
    "id": "SAB4tGfajlSo"
   },
   "outputs": [],
   "source": [
    "s3_resource = boto3.resource(\"s3\", config=Config(signature_version=UNSIGNED))\n",
    "bucket_obj = s3_resource.Bucket(bucket)\n",
    "obj_count = sum(1 for _ in bucket_obj.objects.filter(Prefix=\"v1/data_to_import_neo4j/\"))\n",
    "print(f\"Objects count: {obj_count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RWlDPqOwkuAa",
   "metadata": {
    "id": "RWlDPqOwkuAa"
   },
   "source": [
    "Nodes and edges in one example batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sxwG84j-AkUx",
   "metadata": {
    "id": "sxwG84j-AkUx"
   },
   "outputs": [],
   "source": [
    "for item in s3.list_objects_v2(Bucket=bucket, Prefix=\"v1/data_to_import_neo4j/1737505816_\", MaxKeys=100)[\"Contents\"]:\n",
    "  print(item[\"Key\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "I-LCsw1OBW4y",
   "metadata": {
    "id": "I-LCsw1OBW4y"
   },
   "source": [
    "Next, we download the file containing `script-to-script` edges and its corresponding header file, then create a pandas dataframe using this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QFK2nlWCBuJ-",
   "metadata": {
    "id": "QFK2nlWCBuJ-"
   },
   "outputs": [],
   "source": [
    "with s3.get_object(Bucket=bucket, Key=\"v1/data_to_import_neo4j/1737505816_BitcoinS2S.tsv.gz\")[\"Body\"] as file_object:\n",
    "  df = pd.read_csv(file_object, compression=\"gzip\", sep=\"\\t\")\n",
    "\n",
    "with s3.get_object(Bucket=bucket, Key=\"v1/data_to_import_neo4j/BitcoinS2S_header.tsv.gz\")[\"Body\"] as header_object:\n",
    "  header_series = pd.read_csv(header_object, compression=\"gzip\", sep=\"\\t\", header=None).iloc[0]\n",
    "df.columns = header_series.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-4g-w8N4Dp4B",
   "metadata": {
    "id": "-4g-w8N4Dp4B"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8OXaFGsnFc4Q",
   "metadata": {
    "id": "8OXaFGsnFc4Q"
   },
   "source": [
    "The headers have been adjusted for importing data into a Neo4j database. Each row in the data represents a `script-to-script` edge.\n",
    "\n",
    "* `:START_ID(Script)`: Specifies the ID of the start node and identifies its label as `Script`.\n",
    "\n",
    "* `:END_ID(Script)`: Specifies the ID of the end node and identifies its label as `Script`.\n",
    "\n",
    "* `Value:float`: The transaction amount in BTC. (The `:float` suffix is a type hint for Neo4j; it can be ignored in the dataframe.)`\n",
    "\n",
    "* `Height:int`: The block height of the transaction. (The `:int` suffix is a type hint for Neo4j and can be ignored in the dataframe.)`\n",
    "\n",
    "* `:TYPE`: Sets the edge type in Neo4j.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kn9NPZJdNCkI",
   "metadata": {
    "id": "kn9NPZJdNCkI"
   },
   "source": [
    "## Getting Started with Graph Analysis\n",
    "\n",
    "You can run graph analysis algorithms on this dataset, perhaps by focusing on a time span that's most interesting to your application (i.e., a subset of batches). Since node and edge types are in separate files, you can use a single source (e.g., `script-to-script`) or try combining multiple edge and node types, depending on your application.\n",
    "\n",
    "You can also use this dataset for training graph neural network models. Additionally, since you have `Value` and `Height`, you could build models to predict transfers (like forecasting the number and volume of transfers in future blocks). You could even combine this data with market data to create price prediction models.\n",
    "\n",
    "For demonstration, below we explore some simple summary statistics from the graph using the `networkx` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XuGd_rbtMmKc",
   "metadata": {
    "id": "XuGd_rbtMmKc"
   },
   "outputs": [],
   "source": [
    "# Rename columns to be more intuitive\n",
    "df = df.rename(columns={\n",
    "    \":START_ID(Script)\": \"source\",\n",
    "    \":END_ID(Script)\": \"target\",\n",
    "    \"Value:float\": \"value\",\n",
    "    \"Height:int\": \"height\",\n",
    "    \":TYPE\": \"type\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lMvYpA3cMwPA",
   "metadata": {
    "id": "lMvYpA3cMwPA"
   },
   "outputs": [],
   "source": [
    "# Create a directed graph\n",
    "G = nx.from_pandas_edgelist(df, source=\"source\", target=\"target\", edge_attr=True, create_using=nx.DiGraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qbJviDewM6CE",
   "metadata": {
    "id": "qbJviDewM6CE"
   },
   "outputs": [],
   "source": [
    "print(f\"   Nodes count (script): {G.number_of_nodes():,}\")\n",
    "print(f\"Edges count (transfers): {G.number_of_edges():,}\")\n",
    "print(f\"          Graph density: {nx.density(G):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Iqs3qYiTSD7t",
   "metadata": {
    "id": "Iqs3qYiTSD7t"
   },
   "outputs": [],
   "source": [
    "# Check nodes withest out-degree\n",
    "out_degrees = G.out_degree()\n",
    "sorted_out_degrees = sorted(out_degrees, key=lambda item: item[1], reverse=True)\n",
    "\n",
    "for i, (node, degree) in enumerate(sorted_out_degrees[:3]):\n",
    "    print(f\"{i+1}. Out-degree: {degree:,}\\tScript node: {node}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0kXI_SwLIT9",
   "metadata": {
    "id": "d0kXI_SwLIT9"
   },
   "source": [
    "## Getting Started: Ideas and Directions\n",
    "\n",
    "This raw data already empowers you to build many impactful applications, and because it's batched, you can opt to use only the data relevant to your needs. However, you can develop even more impactful applications by leveraging the deeper insights obtained from loading this data into a graph database, which we cover in the next section.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lzEg5TcwWDOM",
   "metadata": {
    "id": "lzEg5TcwWDOM"
   },
   "source": [
    "# The _Trunk & Roots_ ðŸŒ³ **The Full Graph Database**\n",
    "\n",
    "One of the greatest potentials of the Bitcoin graph is its ability to model long-tail longitudinal trading and analyze node neighborhoods many hops away. This combination of temporal and structural data provides deep context for the transactions and behaviors recorded on the blockchain. Leveraging this potential on a multi-billion node graph, however, requires highly efficient retrieval methods.\n",
    "\n",
    "To deliver on this, we have loaded the batches of nodes and edges (discussed above) into a Neo4j database. We can now leverage Neo4j's capabilities for efficient, deep-context querying of the graph.\n",
    "\n",
    "Importing these batches is a highly computationally intensive process that can take a considerable amount of time (see the [import guide](https://eba.b1aab.ai/docs/bitcoin/etl/import) and [scalability challenges](https://eba.b1aab.ai/docs/gs/accessibility)). To make this resource widely accessible, we provide the database dump. This allows you to skip the bulk import step and simply restore the prepared dataset in Neo4j, which is significantly faster and requires fewer computational resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Q06KwDKEdSrD",
   "metadata": {
    "id": "Q06KwDKEdSrD"
   },
   "outputs": [],
   "source": [
    "bucket=\"bitcoin-graph\"\n",
    "s3 = boto3.client(\"s3\", config=Config(signature_version=UNSIGNED))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KoyMEcgmdeSo",
   "metadata": {
    "id": "KoyMEcgmdeSo"
   },
   "source": [
    "The `neo4j_db_dump/` bucket contains neo4j database dump.\n",
    "\n",
    "Count of objects in this bucket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "B8q_5RCvdeGn",
   "metadata": {
    "id": "B8q_5RCvdeGn"
   },
   "outputs": [],
   "source": [
    "s3_resource = boto3.resource(\"s3\", config=Config(signature_version=UNSIGNED))\n",
    "bucket_obj = s3_resource.Bucket(bucket)\n",
    "obj_count = sum(1 for _ in bucket_obj.objects.filter(Prefix=\"v1/neo4j_db_dump/\"))\n",
    "print(f\"Objects count: {obj_count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "T8GnO8Ixex5h",
   "metadata": {
    "id": "T8GnO8Ixex5h"
   },
   "source": [
    "The database dump is provided as a multi-part `.gz` archive, split into 700 MB partitions. We list a few of the files in this bucket below. You can find instructions on how to load the database into Neo4j on [this documentation page](https://eba.b1aab.ai/docs/bitcoin/etl/import#load)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DTr-pArKe18e",
   "metadata": {
    "id": "DTr-pArKe18e"
   },
   "outputs": [],
   "source": [
    "for item in s3.list_objects_v2(Bucket=bucket, Prefix=\"v1/neo4j_db_dump/neo4j.dump\", MaxKeys=10)[\"Contents\"]:\n",
    "  print(item[\"Key\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oVzGF11Xf1oB",
   "metadata": {
    "id": "oVzGF11Xf1oB"
   },
   "source": [
    "Running the full dataset in Neo4j is highly resource-intensive (requiring ~4TB of storage, [details](https://eba.b1aab.ai/docs/bitcoin/etl/overview)), making it beyond the scope of a _101_ tutorial and impossible to run in a notebook.\n",
    "\n",
    "Therefore, in the next section, we'll work with sampled communities to demonstrate the database's potential. This approach allows you to explore the data's value before committing to the resources needed to run a full database instance yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8zJwu-_k3AY",
   "metadata": {
    "id": "b8zJwu-_k3AY"
   },
   "source": [
    "# The _Branches_ ðŸŒ¿ **Sampled Communities**\n",
    "\n",
    "A common practice for developing machine learning models on such a large graph is to work with sampled communities. However, given that the Bitcoin Graph covers over 16 years of real-world financial trades where trading patterns and volume have significantly evolved, sampling must be application-focused to generate communities that support a specific training objective.\n",
    "\n",
    "As discussed, sampling your own application-specific communities requires a running Neo4j database. You can then follow the steps in [this documentation]((https://eba.b1aab.ai/docs/bitcoin/sampling/overview)) to use the tools EBA provides for this purpose.\n",
    "\n",
    "However, for a quick start that doesn't require hosting a Neo4j database, we have also provided generically sampled communities. We will now go through the steps for accessing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sYIEOAQdnCHl",
   "metadata": {
    "id": "sYIEOAQdnCHl"
   },
   "outputs": [],
   "source": [
    "bucket=\"bitcoin-graph\"\n",
    "s3 = boto3.client(\"s3\", config=Config(signature_version=UNSIGNED))\n",
    "\n",
    "for item in s3.list_objects_v2(Bucket=bucket, Prefix=\"v1/sampled_communities/\", MaxKeys=10)[\"Contents\"]:\n",
    "  print(item[\"Key\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PXzmAd4enkKN",
   "metadata": {
    "id": "PXzmAd4enkKN"
   },
   "source": [
    "The `v1/sampled_communities/script_to_script_200k.zip` archive contains `200 000` communities sampled with the following configuration:\n",
    "\n",
    "* Communities count: `200 000`\n",
    "* Node and edge type: `script-to-script` edges of `transfer` and `fee` types.\n",
    "* Sampling algorithm: _Forest-Fire_ sampling, with the following parameters:\n",
    "  * Node count per community: `[25, 1000]`\n",
    "  * Edge count per community: `[24, 10000]`\n",
    "  * Node count at root: `25`\n",
    "  * Max hops: `3`\n",
    "  * Node count reduction factor by hop: `5`\n",
    "\n",
    "(For a detailed description of these parameters, you may refer to the [paper](https://arxiv.org/abs/2510.20028) or the [method documentation](https://eba.b1aab.ai/docs/bitcoin/sampling/forest-fire).)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aJ29CO4_o8gm",
   "metadata": {
    "id": "aJ29CO4_o8gm"
   },
   "source": [
    "Next, we download and unzip the archive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "v-0eq72IQBgK",
   "metadata": {
    "id": "v-0eq72IQBgK"
   },
   "source": [
    "Note, to simplify accessing the sample graphs to the ML community, we are hosting the sampled communities on both AWS, Kaggle, and Hugging Face. Please choose the resource that works best for you in the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ni4twVQeQGA1",
   "metadata": {
    "id": "Ni4twVQeQGA1"
   },
   "outputs": [],
   "source": [
    "# Please set the following variable.\n",
    "communities_repo=\"kaggle\" #@param [\"kaggle\", \"aws\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qzo7cSi9pCOh",
   "metadata": {
    "id": "qzo7cSi9pCOh"
   },
   "outputs": [],
   "source": [
    "sampled_communities_dir=\"\"\n",
    "if communities_repo == \"kaggle\":\n",
    "  print(\"Downloading from Kaggle ...\")\n",
    "  kaggle_dataset_path = kagglehub.dataset_download(\"aab/bitcoin-graph-sampled-communities/version/1\")\n",
    "  sampled_communities_dir = os.path.join(kaggle_dataset_path, \"script_to_script_200k\")\n",
    "\n",
    "elif communities_repo == \"aws\":\n",
    "  print(\"Downloading from AWS ...\")\n",
    "  with s3.get_object(Bucket=bucket, Key=\"v1/sampled_communities/script_to_script_200k.zip\")[\"Body\"] as file_object:\n",
    "    zip_data = file_object.read()\n",
    "\n",
    "    extract_dir=os.path.join(working_dir, \"sampled_communities\")\n",
    "    with zipfile.ZipFile(io.BytesIO(zip_data)) as zip_file:\n",
    "      zip_file.extractall(path=extract_dir)\n",
    "    sampled_communities_dir=os.path.join(extract_dir, \"script_to_script_200k\")\n",
    "\n",
    "print(f\"Sampled communities downloaded to: {sampled_communities_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HJxxRYBIq4EI",
   "metadata": {
    "id": "HJxxRYBIq4EI"
   },
   "source": [
    "The unzipped directory contains a `raw` directory (for simple loading into `PyG`, as discussed later), which in turn contains a separate subdirectory for each community. See the following tree structure:\n",
    "\n",
    "```shell\n",
    "> tree\n",
    ".\n",
    "â””â”€â”€ script_to_script_200k\n",
    "    â””â”€â”€ raw\n",
    "        â”œâ”€â”€ 202509082048222123\n",
    "        â”‚Â Â  â”œâ”€â”€ BitcoinScriptNode.tsv\n",
    "        â”‚Â Â  â””â”€â”€ metadata.tsv\n",
    "        â”œâ”€â”€ 202509082048237682\n",
    "        â”‚Â Â  â”œâ”€â”€ BitcoinS2S.tsv\n",
    "        â”‚Â Â  â”œâ”€â”€ BitcoinScriptNode.tsv\n",
    "        â”‚Â Â  â””â”€â”€ metadata.tsv\n",
    "        â”œâ”€â”€ 202509082048444566\n",
    "        ...\n",
    "        â””â”€â”€ metadata.tsv\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wwfy175X0-mM",
   "metadata": {
    "id": "wwfy175X0-mM"
   },
   "source": [
    "As an example, we will explore the contents of the `202509082048237682` community."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NhkE3lx60erc",
   "metadata": {
    "id": "NhkE3lx60erc"
   },
   "outputs": [],
   "source": [
    "community_dir=os.path.join(sampled_communities_dir, \"raw\", \"202509082048237682\")\n",
    "os.listdir(community_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ksZHVhqLmN",
   "metadata": {
    "id": "18ksZHVhqLmN"
   },
   "outputs": [],
   "source": [
    "s2s_df = pd.read_csv(os.path.join(community_dir, \"BitcoinS2S.tsv\"), sep=\"\\t\")\n",
    "script_nodes_df = pd.read_csv(os.path.join(community_dir, \"BitcoinScriptNode.tsv\"), sep=\"\\t\")\n",
    "g_metadata_df = pd.read_csv(os.path.join(community_dir, \"metadata.tsv\"), sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oqe3XkQM1cYw",
   "metadata": {
    "id": "oqe3XkQM1cYw"
   },
   "outputs": [],
   "source": [
    "s2s_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2k5clf5a15LV",
   "metadata": {
    "id": "2k5clf5a15LV"
   },
   "outputs": [],
   "source": [
    "script_nodes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_lPgpe3e18KA",
   "metadata": {
    "id": "_lPgpe3e18KA"
   },
   "outputs": [],
   "source": [
    "g_metadata_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sUaBnTcV2CiL",
   "metadata": {
    "id": "sUaBnTcV2CiL"
   },
   "source": [
    "The communities directory also contains a `metadata.tsv` file containing the metadata for all communities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eEV8m2f2MKG",
   "metadata": {
    "id": "2eEV8m2f2MKG"
   },
   "outputs": [],
   "source": [
    "metadata_df = pd.read_csv(\n",
    "    os.path.join(sampled_communities_dir, \"raw\", \"metadata.tsv\"),\n",
    "    sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OIQUhrBY2fU0",
   "metadata": {
    "id": "OIQUhrBY2fU0"
   },
   "outputs": [],
   "source": [
    "metadata_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6-68zZl03EeN",
   "metadata": {
    "id": "6-68zZl03EeN"
   },
   "source": [
    "# The _Fruits_ ðŸŽ **Building Models**\n",
    "\n",
    "\n",
    "In the previous section, we reviewed general-purpose sampled communities. While useful, we highly recommend importing the full graph and using our tools to sample your own. This allows you to tailor the data to your application's specific requirements, which is extremely important.\n",
    "\n",
    "The Bitcoin graph enables you to model entity behavior within deep, contextual neighborhoods. This allows ML models to distinguish entities by their behavioral patterns, largely independent of external data and with high certainty. The deeper and more application-driven the sampling, the more accurate the entity characterization can be (e.g., assigning a trust score based on a wallet's trading patterns). See [this page](https://eba.b1aab.ai/docs/bitcoin/sampling/overview) for details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hwclOkCm6ZWi",
   "metadata": {
    "id": "hwclOkCm6ZWi"
   },
   "source": [
    "However, for this _101_ tutorial, we will use the generic, pre-sampled communities as a showcase. Our goal is to demonstrate a model that learns node (i.e., Bitcoin script/address) embeddings from its neighborhood, which we then use to cluster the nodes.\n",
    "\n",
    "To keep this tutorial simple and fast, we will load pre-trained weights that were generated using these generic communities. This lets us skip the training step and focus on evaluating the model's output. The complete training scripts and steps are discussed in detail on [this page](https://github.com/B1AAB/GraphStudio/tree/main/quickstart/script_classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "js6_6elX3EBJ",
   "metadata": {
    "id": "js6_6elX3EBJ"
   },
   "outputs": [],
   "source": [
    "config = json.load(open(\"/tmp/b1aab/graphstudio/quickstart/script_classification/config.json\"))\n",
    "saves_root = config[\"saves_root\"]\n",
    "out_dim = config[\"out_dim\"]\n",
    "batch_size = config[\"batch_size\"]\n",
    "hidden_channels = config[\"hidden_channels\"]  # note that this has to be divisible by HEADS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ujvg83Si682B",
   "metadata": {
    "id": "ujvg83Si682B"
   },
   "source": [
    "Downloading a trained model, so we do not run training.\n",
    "If interested in re-training the model, you may use\n",
    "[this notebook](https://github.com/B1AAB/GraphStudio/blob/main/quickstart/script_classification/train.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "S0rdEwaA8iHP",
   "metadata": {
    "id": "S0rdEwaA8iHP"
   },
   "outputs": [],
   "source": [
    "model_save_path = os.path.join(working_dir, \"best_encoder.pth\")\n",
    "s3.download_file(bucket, \"v1/g101/best_encoder.pth\", model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OmsAwi_e8zz0",
   "metadata": {
    "id": "OmsAwi_e8zz0"
   },
   "source": [
    "Next we pre-process the \"raw\" sampled communities into a\n",
    "[PyG](https://github.com/pyg-team/pytorch_geometric) dataset.\n",
    "You may refer to the\n",
    "[data loader source code](https://github.com/B1AAB/GraphStudio/blob/main/quickstart/script_classification/script_classification/data_loader.py)\n",
    "for details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Y1__lJHs84E2",
   "metadata": {
    "id": "Y1__lJHs84E2"
   },
   "outputs": [],
   "source": [
    "dataset = BitcoinScriptsDataset(root=sampled_communities_dir)\n",
    "EDGE_DIM = getattr(dataset, \"num_edge_features\", dataset[0].edge_attr.size(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tJOD-3lZ9J0o",
   "metadata": {
    "id": "tJOD-3lZ9J0o"
   },
   "outputs": [],
   "source": [
    "graph_id_to_idx_map = {data.graph_id: i for i, data in enumerate(dataset)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "I5RBHvYA_0vz",
   "metadata": {
    "id": "I5RBHvYA_0vz"
   },
   "outputs": [],
   "source": [
    "n = len(dataset)\n",
    "n_train = int(0.7 * n)\n",
    "n_val   = int(0.15 * n)\n",
    "n_test  = n - n_train - n_val\n",
    "\n",
    "gen = torch.Generator().manual_seed(SEED)\n",
    "train_raw, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    dataset, [n_train, n_val, n_test], generator=gen\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tJiQzm0m_390",
   "metadata": {
    "id": "tJiQzm0m_390"
   },
   "outputs": [],
   "source": [
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blPyGE37_6L5",
   "metadata": {
    "id": "blPyGE37_6L5"
   },
   "outputs": [],
   "source": [
    "encoder = GraphEncoder(\n",
    "    in_channels=dataset.num_node_features,\n",
    "    hidden_channels=hidden_channels,\n",
    "    out_channels=out_dim,\n",
    "    edge_dim=EDGE_DIM\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GzLyGrDj_8hY",
   "metadata": {
    "id": "GzLyGrDj_8hY"
   },
   "outputs": [],
   "source": [
    "ckpt = torch.load(model_save_path, map_location=device)\n",
    "encoder.load_state_dict(ckpt[\"model_state\"])\n",
    "\n",
    "embeddings_for_analysis = embed_roots(encoder, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dqk6j3T_AE50",
   "metadata": {
    "id": "dqk6j3T_AE50"
   },
   "outputs": [],
   "source": [
    "optimal_k, inertias, silhouettes, k_range, best_silhouettes = find_optimal_clusters_root(embeddings_for_analysis, max_k=15, random_state=33)\n",
    "print(f\"Optimal number of clusters: {optimal_k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ifB_9jYOAHvZ",
   "metadata": {
    "id": "ifB_9jYOAHvZ"
   },
   "outputs": [],
   "source": [
    "plot_find_optimal_clusters_root(optimal_k, inertias, silhouettes, k_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dixMDKdpAK07",
   "metadata": {
    "id": "dixMDKdpAK07"
   },
   "outputs": [],
   "source": [
    "def visualize_embeddings_umap(graph_embeddings, cluster_labels, title=\"Graph Embedding Clusters (UMAP)\", figsize=(5, 5)):\n",
    "    reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42, n_jobs=1)\n",
    "    embeddings_2d = reducer.fit_transform(graph_embeddings)\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    scatter = sns.scatterplot(\n",
    "        x=embeddings_2d[:, 0],\n",
    "        y=embeddings_2d[:, 1],\n",
    "        hue=cluster_labels,\n",
    "        palette=\"tab20\",\n",
    "        legend=\"full\",\n",
    "        s=50,\n",
    "        alpha=0.8\n",
    "    )\n",
    "\n",
    "    plt.legend(title=\"Cluster ID\", loc=\"center left\", bbox_to_anchor=(1.02, 0.5), frameon=False)\n",
    "\n",
    "    scatter.set_title(title)\n",
    "    plt.xlabel(\"UMAP Component 1\")\n",
    "    plt.ylabel(\"UMAP Component 2\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LjH1qOfLANOo",
   "metadata": {
    "id": "LjH1qOfLANOo"
   },
   "outputs": [],
   "source": [
    "k_means = KMeans(n_clusters=optimal_k, random_state=11, n_init=10)\n",
    "final_clusters = k_means.fit_predict(embeddings_for_analysis)\n",
    "final_score = silhouette_score(embeddings_for_analysis, final_clusters, metric=\"cosine\")\n",
    "\n",
    "print(f\"Final Silhouette (cosine) with K={optimal_k}: {final_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SZ0suxQrAUIB",
   "metadata": {
    "id": "SZ0suxQrAUIB"
   },
   "source": [
    "Next, we'll visualize the dataset by clustering the embeddings that were generated for the root node of each neighborhood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TMRxVAOpAYF_",
   "metadata": {
    "id": "TMRxVAOpAYF_"
   },
   "outputs": [],
   "source": [
    "visualize_embeddings_umap(embeddings_for_analysis, final_clusters, title=f\"Root Clusters (K={optimal_k})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "r3S5bAPQAeme",
   "metadata": {
    "id": "r3S5bAPQAeme"
   },
   "source": [
    "## Comparing Communities by Graph-Level Features\n",
    "\n",
    "You may also characterize the sampled communities using their graph-level features.\n",
    "These include statistics that summarize the entire sampled subgraph,\n",
    "such as the average block height or BTC value calculated per hop from the community's root node.\n",
    "\n",
    "However, it is crucial to understand a key distinction when using these for evaluation.\n",
    "\n",
    "The model we trained here performs node-level representation learning.\n",
    "It is designed to generate a feature vector (an embedding)\n",
    "for the root node of each sampled community, based on its neighborhood.\n",
    "In contrast, the statistics described above are graph-level\n",
    "features that describe the entire community.\n",
    "\n",
    "Therefore, clustering the root node embeddings and then evaluating\n",
    "those clusters based on graph-level features is a methodological mismatch,\n",
    "and can lead to misleading conclusions.\n",
    "\n",
    "For such a comparison to be valid,\n",
    "the model must be adapted for graph-level representation learning.\n",
    "This typically involves adding a readout or pooling function\n",
    "(e.g., global mean pooling) after the GNN layers\n",
    "to aggregate all node embeddings into a single embedding for the entire graph\n",
    "(or other methods)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-hPC0WcEAoyy",
   "metadata": {
    "id": "-hPC0WcEAoyy"
   },
   "outputs": [],
   "source": [
    "def format_block_height_axis(ax):\n",
    "    formatter = FuncFormatter(lambda x, pos: f'{x / 1000:.0f}k')\n",
    "    ax.xaxis.set_major_formatter(formatter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UkQ4g-KoArFj",
   "metadata": {
    "id": "UkQ4g-KoArFj"
   },
   "outputs": [],
   "source": [
    "def prepare_stats_for_plotting(stats_list, graph_ids):\n",
    "    plot_data = []\n",
    "    for i, stats_dict in enumerate(stats_list):\n",
    "        graph_id = graph_ids[i]\n",
    "        for hop_level, hop_stats in stats_dict.items():\n",
    "            hop_transition_key = f\"hop{hop_level}->hop{hop_level+1}\"\n",
    "\n",
    "            if \"Value_avg\" in hop_stats:\n",
    "                plot_data.append({\n",
    "                    \"graph_id\": graph_id,\n",
    "                    \"hop_transition\": hop_transition_key,\n",
    "                    \"avg_value\": hop_stats[\"Value_avg\"],\n",
    "                    \"metric_type\": \"Avg Value\"\n",
    "                })\n",
    "            if \"BlockHeight_avg\" in hop_stats:\n",
    "                plot_data.append({\n",
    "                    \"graph_id\": graph_id,\n",
    "                    \"hop_transition\": hop_transition_key,\n",
    "                    \"avg_block_height\": hop_stats[\"BlockHeight_avg\"],\n",
    "                    \"metric_type\": \"Avg BlockHeight\"\n",
    "                })\n",
    "            if \"OriginalInDegree_avg\" in hop_stats:\n",
    "                plot_data.append({\n",
    "                    \"graph_id\": graph_id,\n",
    "                    \"hop_transition\": hop_transition_key,\n",
    "                    \"avg_original_in_degree\": hop_stats[\"OriginalInDegree_avg\"],\n",
    "                    \"metric_type\": \"Avg Original In-Degree\"\n",
    "                })\n",
    "            if \"OriginalOutDegree_avg\" in hop_stats:\n",
    "                plot_data.append({\n",
    "                    \"graph_id\": graph_id,\n",
    "                    \"hop_transition\": hop_transition_key,\n",
    "                    \"avg_original_out_degree\": hop_stats[\"OriginalOutDegree_avg\"],\n",
    "                    \"metric_type\": \"Avg Original Out-Degree\"\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(plot_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1ofbTRAuRl",
   "metadata": {
    "id": "3e1ofbTRAuRl"
   },
   "outputs": [],
   "source": [
    "def compare_graph_stats(stats_list, graph_ids):\n",
    "    df = prepare_stats_for_plotting(stats_list, graph_ids)\n",
    "    hop_order = sorted(df[\"hop_transition\"].dropna().unique(), key=lambda x: int(x.split(\"->\")[0][3:]))\n",
    "    color_palette = sns.color_palette(\"viridis\", len(graph_ids))\n",
    "\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(18, 4), sharey=True)\n",
    "\n",
    "    ax1 = axes[0]\n",
    "    ax2 = axes[1]\n",
    "    ax3 = axes[2]\n",
    "    ax4 = axes[3]\n",
    "\n",
    "    sns.barplot(data=df[df[\"metric_type\"] == \"Avg Value\"],\n",
    "                y=\"hop_transition\", x=\"avg_value\", hue=\"graph_id\",\n",
    "                ax=ax1, order=hop_order, orient=\"h\", palette=color_palette)\n",
    "    ax1.set_title(\"Avg. Tx Value by Hop\")\n",
    "    ax1.set_xlabel(\"Average Value (Log10 BTC)\")\n",
    "    ax1.set_ylabel(\"Hop Transition\")\n",
    "    ax1.set_xscale(\"log\")\n",
    "    ax1.get_legend().remove()\n",
    "\n",
    "    sns.barplot(data=df[df[\"metric_type\"] == \"Avg BlockHeight\"],\n",
    "                y=\"hop_transition\", x=\"avg_block_height\", hue=\"graph_id\",\n",
    "                ax=ax2, order=hop_order, orient=\"h\", palette=color_palette)\n",
    "    ax2.set_title(\"Avg. Block Height by Hop\")\n",
    "    ax2.set_xlabel(\"Average Block Height\")\n",
    "    ax2.set_ylabel(\"\")\n",
    "    ax2.get_legend().remove()\n",
    "    format_block_height_axis(ax2)\n",
    "\n",
    "    sns.barplot(data=df[df[\"metric_type\"] == \"Avg Original In-Degree\"],\n",
    "                y=\"hop_transition\", x=\"avg_original_in_degree\", hue=\"graph_id\",\n",
    "                ax=ax3, order=hop_order, orient=\"h\", palette=color_palette)\n",
    "    ax3.set_title(\"Avg. Original In-Degree by Hop\")\n",
    "    ax3.set_xlabel(\"Average Original In-Degree\")\n",
    "    ax3.set_ylabel(\"\")\n",
    "    ax3.get_legend().remove()\n",
    "\n",
    "    sns.barplot(data=df[df[\"metric_type\"] == \"Avg Original Out-Degree\"],\n",
    "                y=\"hop_transition\", x=\"avg_original_out_degree\", hue=\"graph_id\",\n",
    "                ax=ax4, order=hop_order, orient=\"h\", palette=color_palette)\n",
    "    ax4.set_title(\"Avg. Original Out-Degree by Hop\")\n",
    "    ax4.set_xlabel(\"Average Original Out-Degree\")\n",
    "    ax4.set_ylabel(\"\")\n",
    "    ax4.get_legend().remove()\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PoFAtvH7AxSh",
   "metadata": {
    "id": "PoFAtvH7AxSh"
   },
   "outputs": [],
   "source": [
    "def get_graphs_from_cluster(target_cluster_id, all_cluster_labels, dataset):\n",
    "    indices = np.where(all_cluster_labels == target_cluster_id)[0]\n",
    "\n",
    "    if len(indices) == 0:\n",
    "        print(f\"No graphs found for cluster ID {target_cluster_id}.\")\n",
    "        return []\n",
    "\n",
    "    graphs = [dataset[i] for i in indices]\n",
    "    return graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nR00C1bSAz6W",
   "metadata": {
    "id": "nR00C1bSAz6W"
   },
   "outputs": [],
   "source": [
    "a_g_id = get_graphs_from_cluster(1, final_clusters, test_dataset)[0].graph_id\n",
    "b_g_id = get_graphs_from_cluster(1, final_clusters, test_dataset)[1].graph_id\n",
    "compare_graph_stats([dataset.per_graph_stats[a_g_id], dataset.per_graph_stats[b_g_id]], [a_g_id, b_g_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pqyAARryA86X",
   "metadata": {
    "id": "pqyAARryA86X"
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "The model evaluated here is for demonstration purposes only, as are the communities it was trained on. The goal of this **101** is to illustrate what the data looks like and how to work with it.\n",
    "\n",
    "\n",
    "However, given the data's heterogeneity and scale, its 16-year temporal span (during which every aspect has evolved), the complex ETL pipeline involved, and the breadth and depth of potential applications, each use case demands a well-tailored, application-specific solution.\n",
    "\n",
    "\n",
    "For instance, an application studying funds transferred between scripts 2 hops away from the Coinbase node (involving mostly miners) will differ significantly from one studying communities at least 5 hops away (representing mostly user trades).\n",
    "\n",
    "\n",
    "Therefore, we suggest you consider the detailed statistical profile of the blockchain (provided in the manuscript), then design your own sampling and model architecture that best matches your application.\n",
    "\n",
    "\n",
    "On [this page](https://github.com/B1AAB/GraphStudio?tab=readme-ov-file#designing--evaluating-models-on-the-bitcoin-graph),\n",
    "we provide general recommendations for designing applications with this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "khHs8flJnp69",
   "metadata": {
    "id": "khHs8flJnp69"
   },
   "source": [
    "# Your Turn: Take the Next Step!\n",
    "\n",
    "You've seen the basics, but this dataset's real power is in tackling large-scale, real-world challenges. Here are a few high-impact directions you can explore next:\n",
    "\n",
    "### Build On-Chain Reputation Systems\n",
    "\n",
    "Bitcoin is pseudonymous, but behavior isn't. Can you build a GNN that learns an entity's trading patterns (from its deep, temporal neighborhood) to assign it a **\"trust score\"**?\n",
    "\n",
    "Most current methods are either not scalable or rely on hand-tuned rules. A robust, ML-first score could be revolutionary for Decentralized Finance (DeFi) applications, such as enabling under-collateralized lending. You could even use it to power a real-time _\"Internet Security\"_ app for crypto, **flagging transactions to prevent fraud before they are confirmed.**\n",
    "\n",
    "### Stress-Test Your GNN Architecture\n",
    "\n",
    "This graph is a real-world benchmark for GNNs. With over 2.4 billion nodes and ~40 billion edges, it can stress-test your model's scalability at a level you'd normally only see in production---and what better production dataset is there than 16 years of Bitcoin trades?! ðŸ˜€\n",
    "\n",
    "More importantly, its 16-year history contains massive, **real-world temporal distribution shifts.** Can your model generalize, or will a model trained on 2017's data fail on 2025's? This is the perfect dataset to assess your model's robustness and scalability.\n",
    "\n",
    "\n",
    "### Augment with Off-Chain Data\n",
    "\n",
    "We've focused on-chain, but the real magic happens when you combine it with other data. On-chain data gives you the **\"what\"** (the raw facts of a transaction), while off-chain data provides the **\"why\"** (the context, e.g., _\"Was this payment for gambling?\"_). Combining these two worlds is key to boosting the performance of your models.\n",
    "\n",
    "You can augment the graph with market indicators (like OHLC prices) to build more powerful forecasting models. You could also correlate on-chain activity with real-world events (like policy announcements or social media sentiment) to quantify their impact on the ecosystem. We provide an example of using off-chain data on [this page](https://github.com/B1AAB/GraphStudio/tree/main/off_chain_resources), but countless other resources are waiting to be added!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "kn9NPZJdNCkI"
   ],
   "provenance": [
    {
     "file_id": "https://github.com/B1AAB/GraphStudio/blob/main/g101/g101.ipynb",
     "timestamp": 1762902898954
    },
    {
     "file_id": "https://github.com/B1AAB/GraphStudio/blob/main/quickstart/graph_101.ipynb",
     "timestamp": 1762804114442
    }
   ]
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
